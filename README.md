ETL-Pipeline-Crowdfunding

This project presents an efficient approach to data cleaning through the Extract, Transform, Load (ETL) process, facilitating the smooth integration of data into a database. The repository is structured into two primary directories: ETL_Files, containing all essential scripts and resources for executing the ETL process, and Resources, housing the raw CSV files utilized in the project.

Using Python, in conjunction with the Pandas library and Python dictionary methods, the raw data undergoes meticulous extraction and transformation within a Jupyter Notebook environment. The project relies on four CSV files as the basis for constructing an Entity Relationship Diagram (ERD) and developing a comprehensive table schema. Subsequently, the processed data seamlessly transitions into a PostgreSQL database, exemplifying a practical implementation of database management and data integration techniques.

Project Objective:
The primary aim of this project was to harness the capabilities of Python and Pandas, coupled with Python dictionary methods or regular expressions, to construct a robust Extract, Transform, Load (ETL) pipeline. Collaborating with my partner, our objective was to hone essential data engineering skills by extracting, transforming, and loading data. Our journey involved a meticulous transformation of raw data within a Jupyter Notebook environment. We aimed to generate four CSV files from this processed data, serving as the basis for constructing an Entity Relationship Diagram (ERD) and developing a comprehensive table schema. The culmination of our efforts entailed uploading these CSV files into a PostgreSQL database, showcasing our proficiency in efficiently integrating transformed data into a structured database system.

Collaborative Process:
This project delved beyond technical execution; it was an exploration of the collaborative nature of data engineering. We divided tasks, ensuring continual communication and support, which proved pivotal in maintaining a cohesive direction and surmounting challenges. Our focus was on achieving a seamless ETL process, underscoring not only our technical competencies but also the significance of teamwork in data projects.

Outcome and Future:
By project completion, our objective was to establish a fully operational ETL pipeline, serving as a model for future data integration endeavors. This endeavor exemplified practical applications of database management and data integration techniques. It was more than meeting project requirements; it laid the groundwork for advancing data manipulation and analysis skills that I can apply in future projects and professional pursuits.

Key Components:

Category and Subcategory DataFrames Creation
Campaign DataFrame Creation
Contacts DataFrame Creation
Crowdfunding Database Establishment

Repository Structure:
ETL_Files/: Contains scripts and resources essential for the ETL process.
Resources/: Stores raw CSV files pertinent to the project.
ETL_Mini_Project_NRomanoff_JSmith.ipynb: Principal Jupyter notebook for executing the ETL process.

Instructions for Interaction:
Explore data transformations and loading via the provided Jupyter Notebook.
Review CSV exports and Entity Relationship Diagram (ERD) to grasp the database schema.
Explore the processed data within the PostgreSQL database to observe the outcomes of the ETL process.

Data Sources:
Crowdfunding Data: Extracted and transformed from provided Excel files.
Contacts Data: Managed utilizing either Python dictionary methods or regular expressions.
Ethical Considerations:
This project adheres strictly to ethical standards in data handling, prioritizing privacy, accuracy, and transparency throughout the entire ETL process.

